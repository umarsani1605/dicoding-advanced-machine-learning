# -*- coding: utf-8 -*-
"""dicoding-advanced-machine-learning-2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/dicoding-advanced-machine-learning-2-8ce30b04-d44d-40e6-94b2-d15276be5c8a.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20241101/auto/storage/goog4_request%26X-Goog-Date%3D20241101T104046Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D28d6a8a09f3710281fef162a1a4c5d8fb8bfdcc1e44477031be6f2aa68cdecd64ad6d8ade010c458d275a9bb75648ac419562e95f2db988731e150689736dae2e87cb99c9314493e153036a571d5eee0b3806068d94c98ee34576b227d80fab6340d70f7312668f52bf0f7662bd78d4150da5dd8c5b73838538bf5b0a9f0aefa69eae6154241b0172d9cadc3f688c643f14e09397af4a6b16bb5ef8820288cfe80a644afcedaa632f1ebb5579c6b97ac3a292e3c16931648f1aede64c428f184c6b89c90e6d0ad92cbbcca3108e9daa020049139c89a4dce9e4ac8fea3b56dd7fb5cac472315b5e63bbab7b6586bc1a0129054be6217d856aeec988b812f11fe

# Proyek Machine Learning: Recommender System

- **Nama:** Umar Sani
- **Email:** umarsani@student.uns.ac.id
- **ID Dicoding:** umarsani16

## Import Libraries
"""

import string
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from collections import Counter
from ast import literal_eval
from datetime import datetime
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import zipfile
import os, shutil

# set pandas agar tidak memotong kolom
pd.set_option("display.max_columns", 0)

"""## Load Data"""

# Unduh dataset dari Kaggle (jika memakai selain Kaggle Notebook)

# ! kaggle datasets download -d rounakbanik/the-movies-dataset

# # Ekstrak dataset
# def extract(source, destination):
#     zip = zipfile.ZipFile(source, "r")
#     zip.extractall(destination)
#     zip.close()
#     print("[SUCCESS] {} berhasil diekstrak ke '{}'".format(os.path.basename(source), destination))

# source = "/content/the-movies-dataset.zip"
# destination = "/content/the-movies-dataset"

# extract(source, destination)

movies = pd.read_csv('../input/the-movies-dataset/movies_metadata.csv')
keywords = pd.read_csv('../input/the-movies-dataset/keywords.csv')
credits = pd.read_csv('../input/the-movies-dataset/credits.csv')

"""## Data Cleaning and Preprocessing

Pertama, mari kita pahami dataset dengan melihat struktur masing-masing file yang telah di-import. File-file ini nanti akan digabungkan menjadi satu dataframe

### Dataframe Movies

Mari kita lihat 5 baris data teratas, serta informasi jumlah dan tipe dataset.
"""

movies.head()

movies.info()

"""Dari informasi diatas, dapat dilihat ada beberapa kolom yang tipe datanya object. Kolom-kolom tersebut akan kita ubah tipe datanya agar mudah untuk diolah."""

movies['belongs_to_collection'][0]

movies['genres'][0]

movies['production_companies'][0]

movies['production_countries'][0]

movies['spoken_languages'][0]

"""Drop kolom-kolom metadata yang kurang berhubungan dengan informasi movie itu sendiri. Selain itu, terdapat beberapa baris yang bertipe data salah, karena akan membuat error pada saat pengolahan data."""

movies = movies.drop(['belongs_to_collection', 'homepage', 'imdb_id', 'poster_path', 'status', 'title', 'video'], axis=1).drop([19730, 29503, 35587])
movies.head()

"""Sekarang, kita ubah kolom-kolom yang bertipe data object menjadi list."""

def object_to_list(x):
    if isinstance(x, str):
        names = [i['name'] for i in literal_eval(x)]
        # Jika terdapat lebih dari tiga elemen, ambil tiga pertama saja.
        if len(names) > 3:
            names = names[:3]
        return names
    else:
        return []

for column_name in ['production_companies', 'production_countries', 'spoken_languages', 'genres']:
    movies[column_name] = movies[column_name].apply(object_to_list)

movies.head()

"""Sekarang, dataframe movie kita telah lebih rapi.

### Dataframe Keywords

Berikutnya, kita lihat 5 baris data teratas dari dataframe Keywords, serta informasi jumlah dan tipe dataset.
"""

keywords.head()

keywords.info()

keywords['keywords'][0]

"""Sama seperti dataframe Movies, lakukan konversi tipe data object menjadi list pada kolom yang belum sesuai."""

# data prep
keywords['keywords'] = keywords['keywords'].apply(object_to_list)
keywords.head()

"""### Dataframe Credits

Berikutnya, kita lihat 5 baris data teratas dari dataframe Credits, serta informasi jumlah dan tipe dataset.
"""

credits.head()

credits.info()

credits['cast'][0]

credits['crew'][0]

"""Sama seperti dataframe sebelumya, lakukan konversi tipe data object menjadi list pada kolom yang belum sesuai.

Selain itu, untuk kolom crew, kita hanya akan mengambil nama director-nya saja dan men-drop kolom crew itu sendiri.
"""

def get_director(x):
  try:
    crew_list = literal_eval(x)
    director = [member['name'] for member in crew_list if member['job'] == 'Director']
    return director[0]
  except:
    return ''

credits['cast'] = credits['cast'].apply(object_to_list)
credits['director'] = credits['crew'].apply(get_director)

credits = credits.drop(['crew'], axis=1)

credits.head()

credits['cast'][0]

"""### Dataset Gabungkan

Setelah setiap dataframe telah rapi, mari kita gabungkan menjadi satu dataframe.
"""

movies['id'] = movies['id'].astype('int64')

df = movies.merge(keywords, on='id')
df = df.merge(credits, on='id')

df.head()

"""Mari kita cek informasi setiap kolom dataset dan apakah ada nilai null."""

df.info()

df.isnull().sum()

"""Terdapat cukup banyak kolom yang memiliki nilai null.

Untuk kolom dengan tipe string akan kita isi dengan string kosong. Sedangkan kolom dengan tipe numerik akan kita isi dengan nilai mean.
"""

fill_string = ['original_language', 'overview', 'tagline', 'release_date']

for column in fill_string:
  df[column] = df[column].fillna('')

df['popularity'] = df['popularity'].astype('float64')

fill_mean = ['popularity', 'revenue', 'runtime', 'vote_average', 'vote_count']

for column in fill_mean:
  df[column] = df[column].fillna(df[column].mean())

df.isnull().sum()

"""Sekarang, dataset kita telah bersih dari nilai null.

## Data Visualization

### Genre Movie Terbanyak

Dari visualisasi dibawah, kita dapat melihat bahwa genre film dengan jumlah terbanyak adalah drama, diikuti oleh komedi dan aksi.
"""

genres_list = []
for i in df['genres']:
    genres_list.extend(i)

fig = plt.figure(figsize=(16, 6))

df_top_10 = pd.DataFrame(Counter(genres_list).most_common(10), columns=['genre', 'total'])

ax = sns.barplot(data=df_top_10, x='genre', y='total', color='#fb4137')

ax.set_title('10 Genre dengan Film Terbanyak', pad=24)
ax.set_xlabel('Genre')
ax.set_ylabel('Jumlah')

sns.despine()

for p in ax.patches:
    x = p.get_bbox().get_points()[:, 0]
    y = p.get_bbox().get_points()[1, 1]
    ax.annotate('{}'.format(int(y)), (x.mean(), y), ha='center', va='bottom')

"""### Distribusi Film Berdasarkan Tanggal Rilis

Berdasarkan visualisasi di bawah, dapat dilihat bahwa produksi film meningkat pada dekade 2000 hingga sekarang.
"""

df['release_date'] = pd.to_datetime(df['release_date'])

fig = plt.figure(figsize=(16, 6))

sns.displot(data=df, x='release_date', kind='hist', kde=True, color='#18d7ca', facecolor='#fb4137', edgecolor='#fff', aspect=3)

plt.title('Distribusi Film Berdasarkan Tanggal Rilis')

"""### Word Cloud

Berdasarkan word cloud dari plot film, dapat dilihat bahwa kata yang sering muncul adalah "one", "life", "love", "find", "family", dan "live".
"""

plt.figure(figsize=(20,20))

plt.title('Kata dengan Kemunculan Terbanyak')

wc = WordCloud(max_words=100, min_font_size=10, height=600, width=1200, background_color="white", colormap='turbo').generate(' '.join(df['overview']))

plt.imshow(wc)

"""## Content-based filtering

Sekarang, saatnya kita membuat sistem rekomendasi dengan pendekatan Content-based Filtering. Kita akan menggunakan metadata items, seperti genres, keywords, cast, dan director yang terkait dengan masing-masing film.
"""

df[['original_title', 'genres', 'keywords', 'cast', 'director']].head()

"""Pertama, kita akan membersihkan data dengan mengubah semua string menjadi lower case dan menghapus spasi di setiap frasanya."""

def data_cleaning(x):
    if isinstance(x, list):
        return [str.lower(i.replace(" ", "")) for i in x]
    else:
        if isinstance(x, str):
            return str.lower(x.replace(" ", ""))
        else:
            return ''

content_features = ['genres', 'keywords', 'cast', 'director', ]

for feature in content_features:
    df[feature] = df[feature].apply(data_cleaning)

df[['original_title', 'genres', 'keywords', 'cast', 'director']].head()

"""Setelah itu, kita buat kolom baru dengan nama content_filter, yang merupakan string gabungan genres, keywords, cast, dan director."""

def create_content_filter(x):
    return ' '.join(x['genres']) + ' ' + ' '.join(x['keywords']) + ' ' + ' '.join(x['cast']) + ' ' + x['director']

df['content_filter'] = df.apply(create_content_filter, axis=1)
df['content_filter'].head()

df['content_filter'][0]

"""Berikutnya, kita akan melakukan vektorisasi content_filter tersebut dengan Count Vectorizer. Count Vectorizer akan menghitung frekuensi kemunculan kata dalam content_filter dan menghasilkan vektor yang mewakili jumlah kemunculan kata-kata tersebut.

Di bawah ini merupakan visualisasi dari Count Vectorizer (Shah *et al.*, 2020).

![image.png](attachment:944b9885-c6c6-4068-a3d8-9f1c00a9d097.png)

Lakukan fit_transform pada kolom content_filter yang telah dibuat.
"""

cv = CountVectorizer()
vector_matrix = cv.fit_transform(df['content_filter'])

"""Berdasarkan matriks vektor hasil Count Vectorizer, jarak similaritas diantara setiap vektor akan dihitung dengan Cosine Similarity.

Cosine similarity mengukur sudut kosinus antara dua vektor (Neuralis, 2023). Nilai cosine similarity berkisar antara -1 hingga 1, di mana:

- 1 menunjukkan bahwa dua vektor memiliki arah yang sama (sangat mirip).

- 0 menunjukkan bahwa dua vektor ortogonal (tidak ada kesamaan).

- -1 menunjukkan bahwa dua vektor memiliki arah yang berlawanan (sangat tidak mirip).

Di bawah ini merupakan visualisasi dari Cosine Similarity (Neuralis, 2023).

![image.png](attachment:d03417ea-8845-4ea8-9bcf-701b4d9ab1c2.png)
"""

cos_similarity = cosine_similarity(vector_matrix, vector_matrix)

"""(catatan: jika menggunakan Google Colab Free, runtime akan mengalami out of memory)

Setelah kita mendapatkan matriks similaritas, buat fungsi find_recommendations untuk mendapatkan rekomendasi film. Fungsi tersebut akan menghitung jarak similaritas antara film yang diinputkan dengan film-film lainnya. Setelah itu, film-film dengan skor similaritas yang sama akan didapatkan dan diurutkan berdasarkan skor tertinggi. Kemudian, 10 besar film dengan skor similaritas tertinggi akan dikembalikan.
"""

# reset index dataframe agar urut kembali
df = df.reset_index()

def find_recommendations(movies, cos_similarity):
    idx = movies['index'].values[0]
    sim_scores = list(enumerate(cos_similarity[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:11]
    movies_idx = [i[0] for i in sim_scores]
    movies_result = df[['original_title', 'overview', 'genres', 'cast', 'vote_average']].iloc[movies_idx]
    return movies_result.reset_index()

"""Sebagai contoh, mari kita cari rekomendasi film yang mirip dengan film "Toy Story"."""

movie_to_find = df[df['original_title'] == 'Toy Story']

results = find_recommendations(movie_to_find, cos_similarity)

results

"""Dapat kita lihat bahwa sistem rekomendasi kita berhasil membuat rekomendasi film yang mirip dengan "Toy Story".

## Evaluation

Untuk mengukur seberapa baik sistem rekomendasi kita bekerja. Kita akan menggunakan metrik precision. Metrik precision akan menghitung proporsi item yang direkomendasikan yang relevan dengan pengguna.

Metrik precision pada sistem rekomendasi ditunjukkan oleh formula berikut.

$Precision = \frac{\text{Jumlah item relevan yang direkomendasikan}}{\text{Jumlah total item yang direkomendasikan}} $

Jumlah item relevan dalam percobaan ini kita tentukan sebagai jumlah film dengan genre yang mirip.

Jumlah total item adalah jumlah film yang direkomendasikan oleh sistem kita.

Sebagai contoh, mari kita buat rekomendasi film berdasarkan film "Star Wars"
"""

movie_to_find = df[df['original_title'] == 'Star Wars']

results = find_recommendations(movie_to_find, cos_similarity)

results

"""Kita telah mendapatkan 10 rekomendasi film yang mirip.

Untuk mendapatkan jumlah film yang relevan, kita akan menghitung similaritas genre antara movie_to_find dengan film-film hasil rekomendasi.
"""

movie_to_find['genres'].tolist()[0]

results['genres'].tolist()

"""Untuk menghitung similaritas genre antara satu film dengan film yang lain. Kita akan menggunakan jaccard similarity. Jackard similarity digunakan untuk mengukur tingkat kemiripan antara dua himpunan.

Jackard similarity ditunjukkan oleh formula berikut.

$ J(A, B) = \frac{|A \cap B|}{|A \cup B|} $

$ dengan: $

$ A = Set1 $

$ B = Set2 $

Mari kita buat fungsi untuk menghitung jaccard similarity
"""

def jaccard_similarity(list1, list2):
    set1 = set(list1)
    set2 = set(list2)
    intersection = set1.intersection(set2)
    union = set1.union(set2)
    return len(intersection) / len(union)

"""Berikutnya, mari kita lakukan perhitungan jaccard similarity dari film yang ingin dicari dengan film-film hasil rekomendasi."""

list_1 = movie_to_find['genres'].tolist()[0]
list_2 = results['genres'].tolist()

similarities = []

for result in list_2:
  jack_similarity = jaccard_similarity(list_1, result)
  similarities.append(jack_similarity)

jackard_result = pd.DataFrame({'Movie': range(1, len(results) + 1), 'Jaccard Similarity': similarities})

jackard_result

"""Kita akan tentukan film yang relevan adalah film dengan similaritas diatas 0.8. Setelah itu, jumlah dari film yang relevan akan kita bagi dengan jumlah film rekomendasi untuk mendapatkan precision."""

relevan_movie = jackard_result[jackard_result['Jaccard Similarity'] >= 0.8]

precision = len(relevan_movie)/len(results)

print(f"Precision: {precision*100}%")

"""Berdasarkan perhitungan diatas, kita mendapatkan precision sebesar 90%. Hasil tersebut sudah cukup bagus untuk sebuah sistem rekomendasi.

## Referensi

Neuralis. (2023, February 25). How Cosine similarity can improve your machine learning Models - AITechTrend. AITechTrend - Further into the Future. https://aitechtrend.com/how-cosine-similarity-can-improve-your-machine-learning-models/

Shah, S. R., Kaushik, A., Sharma, S., & Shah, J. (2020). Opinion-mining on marglish and devanagari comments of youtube cookery channels using parametric and non-parametric learning models. Big Data and Cognitive Computing, 4(1), 3.
"""